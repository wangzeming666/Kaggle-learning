{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ae08ad96",
   "metadata": {},
   "source": [
    "# 📘 深度理解三大经典工具：PCA / t-SNE / KMeans\n",
    "\n",
    "本笔记涵盖：\n",
    "- 每个方法的原理与推导\n",
    "- 使用时机与判断依据\n",
    "- 数学公式与直觉解释\n",
    "- Python 使用代码模板\n",
    "- 适用与不适用场景"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7441cdf7",
   "metadata": {},
   "source": [
    "## 🔷 1. 主成分分析（PCA）\n",
    "\n",
    "### ✅ 适用场景：\n",
    "- 数值特征维度高，存在共线性、冗余\n",
    "- 希望压缩数据维度、加速模型\n",
    "- 可视化高维数据结构\n",
    "\n",
    "### 🧠 原理：\n",
    "PCA 的目标是找到投影方向，使得投影后数据的方差最大。这些方向称为“主成分”，它们是协方差矩阵的特征向量。\n",
    "\n",
    "### 📐 数学流程：\n",
    "1. 零均值化：$\\tilde{X} = X - \\bar{X}$\n",
    "2. 协方差矩阵：$C = \\frac{1}{n} \\tilde{X}^T \\tilde{X}$\n",
    "3. 求特征值与特征向量：$C = U\\Lambda U^T$\n",
    "4. 选取前 $k$ 个主成分：$Z = \\tilde{X}U_k$\n",
    "\n",
    "### 🧪 判断依据：\n",
    "- `np.sum(pca.explained_variance_ratio_) > 0.95` 说明压缩后信息保留足够\n",
    "\n",
    "### 💻 示例代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d34591",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "X_scaled = StandardScaler().fit_transform(X)\n",
    "pca = PCA(n_components=0.95)  # 保留95%方差\n",
    "X_pca = pca.fit_transform(X_scaled)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79e99608",
   "metadata": {},
   "source": [
    "## 🔶 2. t-SNE（非线性降维）\n",
    "\n",
    "### ✅ 适用场景：\n",
    "- 想在 2D 或 3D 图上“看见”高维结构\n",
    "- 分析聚类效果、数据群组\n",
    "\n",
    "### ⚠️ 注意：\n",
    "- t-SNE 是可视化工具，不适合用于训练或预测\n",
    "- 对噪声敏感，不能保留全局结构\n",
    "\n",
    "### 🧠 原理：\n",
    "t-SNE 构造高维中相邻点的概率分布（高斯），在低维中用 t 分布重建这种邻近关系，通过最小化 KL 散度实现。\n",
    "\n",
    "### 📐 关键公式：\n",
    "- 高维邻接概率：\n",
    "$p_{j|i} = \\frac{e^{-\\|x_i - x_j\\|^2 / 2\\sigma_i^2}}{\\sum_{k \\ne i} e^{-\\|x_i - x_k\\|^2 / 2\\sigma_i^2}}$\n",
    "- 低维相似度：$q_{ij} \\propto (1 + \\|y_i - y_j\\|^2)^{-1}$\n",
    "- KL 散度优化目标：$KL(P\\|Q) = \\sum p_{ij} \\log \\frac{p_{ij}}{q_{ij}}$\n",
    "\n",
    "### 💻 示例代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f23e933",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.manifold import TSNE\n",
    "X_embedded = TSNE(n_components=2, perplexity=30).fit_transform(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd47afa8",
   "metadata": {},
   "source": [
    "## 🔵 3. KMeans 聚类\n",
    "\n",
    "### ✅ 适用场景：\n",
    "- 无监督任务中寻找簇结构\n",
    "- 客户/用户/样本分群\n",
    "- 特征工程中构造 cluster id 作为伪标签\n",
    "\n",
    "### 🧠 原理：\n",
    "KMeans 通过迭代最小化簇内平方误差（inertia），每个点分配给最近中心。\n",
    "\n",
    "### 📐 数学模型：\n",
    "- 优化目标：$J = \\sum_k \\sum_{x_i \\in C_k} \\|x_i - \\mu_k\\|^2$\n",
    "- 每轮迭代：\n",
    "  - E-step：指定每个点到最近中心\n",
    "  - M-step：更新中心为簇内平均值\n",
    "\n",
    "### 💡 判断依据：\n",
    "- 肘部法估计 k 值：绘制 inertia vs k 折线图\n",
    "\n",
    "### 💻 示例代码："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "86fd5d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "kmeans = KMeans(n_clusters=5).fit(X)\n",
    "labels = kmeans.labels_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12830459",
   "metadata": {},
   "source": [
    "## 📊 三者比较总结：\n",
    "| 方法 | 类型 | 用途 | 可视化 | 预测能力 | 数学核心 |\n",
    "|------|------|------|----------|------------|------------|\n",
    "| PCA | 线性降维 | 压缩数据、去冗余 | ✅ | ✅ | 特征值分解 |\n",
    "| t-SNE | 非线性降维 | 可视化群组结构 | ✅✅ | ❌ | 概率邻接 + KL 散度 |\n",
    "| KMeans | 聚类 | 无监督分群 | 可配合可视化 | ❌（非监督） | 最小平方距离 |\n",
    "\n",
    "### 🧭 建议使用顺序：\n",
    "1. 先用 PCA 预压缩高维数据\n",
    "2. 用 KMeans 找聚类结构\n",
    "3. 用 t-SNE 将聚类结果可视化"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
