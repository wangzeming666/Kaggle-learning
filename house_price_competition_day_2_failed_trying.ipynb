{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4d7f09a1-0da2-40bc-874e-56648b89a475",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-07-08 08:10:31.083737: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:467] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1751929831.099140  117604 cuda_dnn.cc:8579] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1751929831.103615  117604 cuda_blas.cc:1407] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n",
      "W0000 00:00:1751929831.114951  117604 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751929831.114967  117604 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751929831.114968  117604 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "W0000 00:00:1751929831.114970  117604 computation_placer.cc:177] computation placer already registered. Please check linkage and avoid linking the same target more than once.\n",
      "2025-07-08 08:10:31.118366: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "from scipy.stats import skew  \n",
    "from scipy.special import boxcox1p\n",
    "from scipy.stats import boxcox_normmax\n",
    "\n",
    "from sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from mlxtend.regressor import StackingCVRegressor\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import umap.umap_ as umap\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from lightgbm import LGBMRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.ensemble import StackingRegressor\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import category_encoders as ce\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "\n",
    "import hdbscan\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a0493d-ae62-402d-92da-6a0e7bcb036d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train set size: (200000, 47)\n",
      "Test set size: (200000, 46)\n",
      "START data processing 2025-07-08 08:10:36.000141\n",
      "(400000, 45)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/wangzeming/.local/lib/python3.10/site-packages/umap/umap_.py:1952: UserWarning: n_jobs value 1 overridden to 1 by setting random_state. Use no seed for parallelism.\n",
      "  warn(\n",
      "/home/wangzeming/.local/lib/python3.10/site-packages/numba/np/ufunc/parallel.py:371: NumbaWarning: \u001b[1mThe TBB threading layer requires TBB version 2021 update 6 or later i.e., TBB_INTERFACE_VERSION >= 12060. Found TBB_INTERFACE_VERSION = 12050. The TBB threading layer is disabled.\u001b[0m\n",
      "  warnings.warn(problem)\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('house_price/dataset.csv')\n",
    "test = pd.read_csv('house_price/test.csv')\n",
    "print(\"Train set size:\", train.shape)\n",
    "print(\"Test set size:\", test.shape)\n",
    "print('START data processing', datetime.now(), )\n",
    "\n",
    "train_ID = train['id']\n",
    "test_ID = test['id']\n",
    "# Now drop the  'Id' colum since it's unnecessary for  the prediction process.\n",
    "train.drop(['id'], axis=1, inplace=True)\n",
    "test.drop(['id'], axis=1, inplace=True)\n",
    "\n",
    "# Deleting outliers\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\n",
    "train[\"sale_price\"] = np.log1p(train[\"sale_price\"])\n",
    "y = train.sale_price.reset_index(drop=True)\n",
    "train_features = train.drop(['sale_price'], axis=1)\n",
    "test_features = test\n",
    "\n",
    "features = pd.concat([train_features, test_features]).reset_index(drop=True)\n",
    "print(features.shape)\n",
    "# Some of the non-numeric predictors are stored as numbers; we convert them into strings \n",
    "\n",
    "\n",
    "# === 缺失值处理 === #\n",
    "def preprocess_and_encode(df, y=None, drop_high_card=True):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # 合并城市 + subdivision 做原始地址字符串\n",
    "    addresses = df['city'].fillna('') + ' ' + df['subdivision'].fillna('').str.lower()\n",
    "\n",
    "    # TF-IDF 特征（可以控制 max_features 降维）\n",
    "    vectorizer = TfidfVectorizer(max_features=300, stop_words='english')\n",
    "    address_vecs = vectorizer.fit_transform(addresses)\n",
    "    \n",
    "    # 降维\n",
    "    umap_vecs = umap.UMAP(n_components=2, random_state=42).fit_transform(address_vecs)\n",
    "    \n",
    "    # 聚类\n",
    "    kmeans = KMeans(n_clusters=30, random_state=42)\n",
    "    labels = kmeans.fit_predict(umap_vecs)\n",
    "    \n",
    "    # 加入 DataFrame 可视化\n",
    "    df['addr_cluster'] = labels.astype(str)\n",
    "\n",
    "    \n",
    "    fr = pd.DataFrame({'umap_vecs1': umap_vecs[:,0], 'umap_vecs2': umap_vecs[:, 1], 'cluster': kmeans.labels_})\n",
    "    sns.lmplot(data=fr, x='umap_vecs1', y='umap_vecs2', hue='cluster', fit_reg=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # 地址字符串（city + subdivision）\n",
    "    addresses = (df['city'].fillna('') + ' ' + df['subdivision'].fillna('')).str.lower()\n",
    "    \n",
    "    # 向量化\n",
    "    vec = TfidfVectorizer(max_features=500, stop_words='english')\n",
    "    X_text = vec.fit_transform(addresses)\n",
    "    \n",
    "    # 降维\n",
    "    X_umap = umap.UMAP(\n",
    "        n_neighbors=10,       \n",
    "        min_dist=0.1,          \n",
    "        n_components=5,       \n",
    "        metric='cosine',        \n",
    "        random_state=42\n",
    "    ).fit_transform(X_text)\n",
    "    \n",
    "    # 聚类\n",
    "    clusterer = hdbscan.HDBSCAN(\n",
    "        min_cluster_size=20,       \n",
    "        min_samples=5,           \n",
    "        cluster_selection_epsilon=0.1\n",
    "    )\n",
    "    \n",
    "    labels = clusterer.fit_predict(X_umap)\n",
    "    \n",
    "    df['addr_cluster222'] = labels.astype(str)     \n",
    "\n",
    "    \n",
    "    # ================= 清洗阶段 ================= #\n",
    "    # 补充缺失类别为 'None'\n",
    "    cat_cols = df.select_dtypes(include='object').columns\n",
    "    df[cat_cols] = df[cat_cols].fillna('None')\n",
    "\n",
    "    # 填充数值缺失为 0\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    df[num_cols] = df[num_cols].fillna(0)\n",
    "\n",
    "    # 拆解日期特征\n",
    "    if 'sale_date' in df.columns:\n",
    "        df['sale_year'] = pd.to_datetime(df['sale_date']).dt.year\n",
    "        df['sale_month'] = pd.to_datetime(df['sale_date']).dt.month\n",
    "        df = df.drop(columns='sale_date')\n",
    "\n",
    "    # subdivision 用 target encoding 处理\n",
    "    if 'subdivision' in df.columns and y is not None:\n",
    "        encoder_sub = ce.TargetEncoder()\n",
    "        df['subdivision_encoded'] = encoder_sub.fit_transform(df['subdivision'], y)\n",
    "        df = df.drop(columns='subdivision')\n",
    "\n",
    "    # ================= 编码阶段 ================= #\n",
    "    cat_cols = df.select_dtypes(include='object').columns\n",
    "    low_card_cols = [col for col in cat_cols if df[col].nunique() <= 50]\n",
    "    high_card_cols = [col for col in cat_cols if df[col].nunique() > 50]\n",
    "\n",
    "    # One-hot 编码\n",
    "    X_cat = pd.get_dummies(df[low_card_cols], dummy_na=True)\n",
    "\n",
    "    # Target 编码\n",
    "    if y is not None and high_card_cols:\n",
    "        encoder = ce.TargetEncoder()\n",
    "        X_target = encoder.fit_transform(df[high_card_cols], y)\n",
    "    else:\n",
    "        X_target = pd.DataFrame(index=df.index)\n",
    "\n",
    "    # 数值标准化\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    scaler = StandardScaler()\n",
    "    X_num_scaled = pd.DataFrame(scaler.fit_transform(df[num_cols]), columns=num_cols, index=df.index)\n",
    "\n",
    "    # 拼接所有\n",
    "    X_final_df = pd.concat([X_num_scaled, X_cat, X_target], axis=1)\n",
    "\n",
    "    return X_final_df\n",
    "\n",
    "# 应用预处理\n",
    "X_final = preprocess_and_encode(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb16a76c-27a1-4833-bc6a-4c29f861f368",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "197525fa-364f-45f5-ac78-4cefc271de01",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X_final.iloc[:len(train_features)].reset_index(drop=True)\n",
    "X_sub = X_final.iloc[len(train_features):].reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d394f02-9fc1-4e6b-9984-f9483d59ed0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('X', X.shape, 'y', y.shape, 'X_sub', X_sub.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af42aad2-3a64-4a35-b99d-394dac1a6dc1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# outliers = [30, 88, 462, 631, 1322]\n",
    "# X = X.drop(X.index[outliers])\n",
    "# y = y.drop(y.index[outliers])\n",
    "\n",
    "\n",
    "protected_cols = ['addr_cluster', 'addr_cluster222']\n",
    "overfit = [i for i in X.columns if i not in protected_cols and X[i].value_counts().iloc[0] / len(X) * 100 > 99.94]\n",
    "\n",
    "\n",
    "print(overfit)\n",
    "\n",
    "# X = X.drop(overfit, axis=1, errors='ignore')\n",
    "# X_sub = X_sub.drop(overfit, axis=1, errors='ignore')\n",
    "X = X.drop(overfit, axis=1)\n",
    "X_sub = X_sub.drop(overfit, axis=1)\n",
    "\n",
    "print('X', X.shape, 'y', y.shape, 'X_sub', X_sub.shape)\n",
    "\n",
    "# ################## ML ########################################\n",
    "print('START ML', datetime.now(), )\n",
    "\n",
    "kfolds = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "\n",
    "\n",
    "# rmsle\n",
    "def rmsle(y, y_pred):\n",
    "    return np.sqrt(mean_squared_error(y, y_pred))\n",
    "\n",
    "\n",
    "# build our model scoring function\n",
    "def cv_rmse(model, X=X):\n",
    "    rmse = np.sqrt(-cross_val_score(model, X, y,\n",
    "                                    scoring=\"neg_mean_squared_error\",\n",
    "                                    cv=kfolds))\n",
    "    return (rmse)\n",
    "\n",
    "\n",
    "# setup models    \n",
    "alphas_alt = [14.5, 14.6, 14.7, 14.8, 14.9, 15, 15.1, 15.2, 15.3, 15.4, 15.5]\n",
    "alphas2 = [5e-05, 0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007, 0.0008]\n",
    "e_alphas = [0.0001, 0.0002, 0.0003, 0.0004, 0.0005, 0.0006, 0.0007]\n",
    "e_l1ratio = [0.8, 0.85, 0.9, 0.95, 0.99, 1]\n",
    "\n",
    "ridge = make_pipeline(RobustScaler(),\n",
    "                      RidgeCV(alphas=alphas_alt, cv=kfolds))\n",
    "\n",
    "lasso = make_pipeline(RobustScaler(),\n",
    "                      LassoCV(max_iter=int(5000), alphas=alphas2,\n",
    "                              random_state=42, cv=kfolds))\n",
    "\n",
    "elasticnet = make_pipeline(RobustScaler(),\n",
    "                           ElasticNetCV(max_iter=int(5000), alphas=e_alphas,\n",
    "                                        cv=kfolds, l1_ratio=e_l1ratio))\n",
    "                                        \n",
    "svr = make_pipeline(RobustScaler(),\n",
    "                      SVR(C=1.0, epsilon=0.1, gamma='scale'))\n",
    "\n",
    "\n",
    "gbr = GradientBoostingRegressor(n_estimators=1000, learning_rate=0.05,\n",
    "                                   max_depth=4, max_features='sqrt',\n",
    "                                   min_samples_leaf=15, min_samples_split=10, \n",
    "                                   loss='huber', random_state =42)\n",
    "                                   \n",
    "\n",
    "#lightgbm = LGBMRegressor(objective='regression', n_jobs=-1,\n",
    "#                                       num_leaves=4,\n",
    "#                                       learning_rate=0.01, \n",
    "#                                       n_estimators=5000,\n",
    "#                                       max_bin=200, \n",
    "#                                       bagging_fraction=0.75,\n",
    "#                                       bagging_freq=5, \n",
    "#                                       bagging_seed=7,\n",
    "#                                       feature_fraction=0.2,\n",
    "#                                       feature_fraction_seed=7,\n",
    "#                                       verbose=-1,\n",
    "                                       #min_data_in_leaf=2,\n",
    "                                       #min_sum_hessian_in_leaf=11\n",
    "#                                       )\n",
    "\n",
    "lightgbm = LGBMRegressor(\n",
    "    objective='regression',\n",
    "    n_estimators=1000,\n",
    "    learning_rate=0.05,\n",
    "    num_leaves=15,\n",
    "    max_depth=5,\n",
    "    subsample=0.8,\n",
    "    colsample_bytree=0.8,\n",
    "    random_state=42,\n",
    "    n_jobs=-1\n",
    ")\n",
    "                                       \n",
    "\n",
    "xgboost = XGBRegressor(\n",
    "    learning_rate=0.05, \n",
    "    n_estimators=1000,   \n",
    "    max_depth=5, min_child_weight=1,\n",
    "    gamma=0, subsample=0.7,\n",
    "    colsample_bytree=0.7,\n",
    "    objective='reg:squarederror', n_jobs=-1,\n",
    "    reg_alpha=0.0001, seed=42\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "                                \n",
    "\n",
    "print('TEST score on CV')\n",
    "\n",
    "#score = cv_rmse(ridge)\n",
    "#print(\"Kernel Ridge score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n",
    "\n",
    "#score = cv_rmse(lasso)\n",
    "#print(\"Lasso score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n",
    "\n",
    "#score = cv_rmse(elasticnet)\n",
    "#print(\"ElasticNet score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n",
    "\n",
    "#score = cv_rmse(svr)\n",
    "#print(\"SVR score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n",
    "\n",
    "score = cv_rmse(lightgbm)\n",
    "print(\"Lightgbm score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n",
    "\n",
    "score = cv_rmse(gbr)\n",
    "print(\"GradientBoosting score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n",
    "\n",
    "score = cv_rmse(xgboost)\n",
    "print(\"Xgboost score: {:.4f} ({:.4f})\\n\".format(score.mean(), score.std()), datetime.now(), )\n",
    "\n",
    "print('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cab1c8c4-5921-4ae2-b0c8-974c7912f433",
   "metadata": {},
   "outputs": [],
   "source": [
    "print('START Fit')\n",
    "#print(datetime.now(), 'StackingRegressor')\n",
    "#stack_gen_model = stack_gen.fit(X, y)\n",
    "print(datetime.now(), 'elasticnet')\n",
    "elastic_model_full_data = elasticnet.fit(X, y)\n",
    "print(datetime.now(), 'lasso')\n",
    "lasso_model_full_data = lasso.fit(X, y)\n",
    "print(datetime.now(), 'ridge')\n",
    "ridge_model_full_data = ridge.fit(X, y)\n",
    "print(datetime.now(), 'svr')\n",
    "svr_model_full_data = svr.fit(X, y)\n",
    "print(datetime.now(), 'GradientBoosting')\n",
    "gbr_model_full_data = gbr.fit(X, y)\n",
    "print(datetime.now(), 'xgboost')\n",
    "xgb_model_full_data = xgboost.fit(X, y)\n",
    "print(datetime.now(), 'lightgbm')\n",
    "lgb_model_full_data = lightgbm.fit(X, y)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
