{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e13a923-c075-4f83-a77b-97756c357886",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['house_price_competition_day_2_failed_trying.ipynb', 'house_price_advanced_view.ipynb', 'addr_kmeans.pkl', 'submission.csv', 'house_price_advanced_models.ipynb', 'my_model_submission.csv1', 'my_model_submission4.csv', 'house_price', 'addr_umap.pkl', 'Day1.ipynb', 'titanic', 'house-prices-advanced-regression-techniques.zip', 'titanic.zip', 'my_model_submission3.csv', 'house_price_competition_view_day2.ipynb', 'Untitled.ipynb', 'my_model_submission0.csv', 'X_umap.npy', 'addr_tfidf.pkl', 'Day2 Housing Price.ipynb', 'pca_model.pkl', 'my_model_submission.csv', 'predictions.csv', 'umap_model.pkl', 'predictions4.csv', 'my_model_submission1.csv', 'competition_day7.ipynb', 'Untitled1.ipynb', 'house_price_view.ipynb', 'home-data-for-ml-course.zip', '.ipynb_checkpoints', 'home-data-for-ml-course', 'house_price.ipynb', 'predictions3.csv', 'my_model_submission2.csv', 'best_interval_model.pth', 'predictions2.csv']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from datetime import datetime\n",
    "from scipy.stats import skew \n",
    "from scipy.special import boxcox1p\n",
    "from scipy.stats import boxcox_normmax\n",
    "from sklearn.linear_model import ElasticNetCV, LassoCV, RidgeCV\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from mlxtend.regressor import StackingCVRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from lightgbm import LGBMRegressor\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy.stats as stats\n",
    "import sklearn.linear_model as linear_model\n",
    "import seaborn as sns\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.neighbors import KDTree\n",
    "from sklearn.decomposition import TruncatedSVD\n",
    "from sklearn.neighbors import BallTree\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "\n",
    "import os\n",
    "print(os.listdir())\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "02cf2a5d-624b-4d1d-b4bb-4de1b22b15dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data is loaded!\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv('house_price/dataset.csv')\n",
    "test = pd.read_csv('house_price/test.csv')\n",
    "print (\"Data is loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba3be9cc-7f7e-4a2b-aa2a-9f9b4254d0c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "quantitative = [f for f in train.columns if train.dtypes[f] != 'object']\n",
    "quantitative.remove('sale_price')\n",
    "quantitative.remove('id')\n",
    "qualitative = [f for f in train.columns if train.dtypes[f] == 'object']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a1eb4182-b5d3-4294-b065-94375e68adcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sale_nbr       42182\n",
      "subdivision    17550\n",
      "submarket       1717\n",
      "dtype: int64\n",
      "(200000, 46)\n",
      "(200000, 45)\n"
     ]
    }
   ],
   "source": [
    "def dataset_fill_null(obj):\n",
    "    obj['subdivision'].fillna('Unknown', inplace=True)\n",
    "    obj.drop(columns=['sale_nbr'], inplace=True)\n",
    "    obj['submarket'].fillna('Unknown', inplace=True)\n",
    "\n",
    "missing = train.isnull().sum()\n",
    "missing = missing[missing > 0]\n",
    "print(missing)\n",
    "\n",
    "dataset_fill_null(train)\n",
    "dataset_fill_null(test)\n",
    "print(train.shape)\n",
    "print(test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "70184b3f-80d5-46ec-9026-5776562f0664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构造原始地址字段\n",
    "train_ID = train['id']\n",
    "test_ID = test['id']\n",
    "# Now drop the  'Id' colum since it's unnecessary for  the prediction process.\n",
    "train.drop(['id'], axis=1, inplace=True)\n",
    "test.drop(['id'], axis=1, inplace=True)\n",
    "# Deleting outliers\n",
    "train.reset_index(drop=True, inplace=True)\n",
    "# We use the numpy fuction log1p which  applies log(1+x) to all elements of the column\n",
    "train[\"sale_price\"] = np.log1p(train[\"sale_price\"])\n",
    "y = train.sale_price.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "baebddd0-786d-4388-802d-d94f8b432c95",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_and_encode(df, y=None, drop_high_card=True):\n",
    "    df = df.copy()\n",
    "    \n",
    "    # ================= 清洗阶段 ================= #\n",
    "    addresses = (df['city'].fillna('') + ' ' + df['subdivision'].fillna('')).str.lower()\n",
    "    \n",
    "    # 分词（空格切）\n",
    "    # 用 CountVectorizer 可以保留频率稀疏性\n",
    "    \n",
    "    vectorizer = CountVectorizer(\n",
    "        max_features=1000,         # 可调大小\n",
    "        stop_words=None,      # 去常见词\n",
    "        token_pattern=r'\\b\\w+\\b',  # 标准单词\n",
    "        ngram_range=(1, 2)         # 一元和二元组都试试\n",
    "    )\n",
    "    address_vecs = vectorizer.fit_transform(addresses)\n",
    "    \n",
    "    address_df = pd.DataFrame(address_vecs.toarray(), columns=vectorizer.get_feature_names_out())\n",
    "    address_df.index = df.index\n",
    "    df = pd.concat([df, address_df], axis=1)\n",
    "    df = df.drop(columns=['city', 'subdivision'])\n",
    "\n",
    "    # 使用同一个 address_vecs（CountVectorizer 输出）\n",
    "    svd = TruncatedSVD(n_components=50, random_state=42)\n",
    "    address_pca = svd.fit_transform(address_vecs)\n",
    "    \n",
    "    # 转成 DataFrame 并拼接\n",
    "    pca_df = pd.DataFrame(address_pca, columns=[f'address_pca_{i+1}' for i in range(address_pca.shape[1])], index=df.index)\n",
    "    df = pd.concat([df, pca_df], axis=1)\n",
    "\n",
    "    # 补充缺失类别为 'None'\n",
    "    cat_cols = df.select_dtypes(include=['object', 'category', 'string']).columns\n",
    "    \n",
    "    for col in cat_cols:\n",
    "        df[col] = df[col].fillna('None')\n",
    "\n",
    "    # 填充数值缺失为 0\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    for col in num_cols:\n",
    "        df[col] = df[col].fillna(0)\n",
    "\n",
    "    # 拆解日期特征\n",
    "    if 'sale_date' in df.columns:\n",
    "        df['sale_date'] = pd.to_datetime(df['sale_date'])\n",
    "        df['sale_year'] = pd.to_datetime(df['sale_date']).dt.year\n",
    "        df['sale_month'] = pd.to_datetime(df['sale_date']).dt.month\n",
    "        df['house_age'] = df['sale_year'] - df['year_built']\n",
    "        df['reno_age'] = df['sale_year'] - df['year_reno']\n",
    "        df['has_reno'] = (df['year_reno'] > 0).astype(int)\n",
    "        df['land_imp_ratio'] = df['land_val'] / (df['imp_val'] + 1e-5)\n",
    "        df['time_since_sale'] = (df['sale_date'].max() - df['sale_date']).dt.days / 30  # 单位：月        \n",
    "        df = df.drop(columns='sale_date')\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # ================= 编码阶段 ================= #\n",
    "    cat_cols = df.select_dtypes(include='object').columns\n",
    "    low_card_cols = []\n",
    "    high_card_cols = []\n",
    "    \n",
    "    for col in cat_cols:\n",
    "        try:\n",
    "            n_unique = df[col].nunique()\n",
    "            if isinstance(n_unique, (int, np.integer)):\n",
    "                if n_unique <= 50:\n",
    "                    low_card_cols.append(col)\n",
    "                else:\n",
    "                    high_card_cols.append(col)\n",
    "            else:\n",
    "                print(f\"[跳过] {col} 的 nunique 结果不是标量: {n_unique}\")\n",
    "        except Exception as e:\n",
    "            print(f\"[异常] {col}: {e}\")\n",
    "\n",
    "\n",
    "    # One-hot 编码\n",
    "    X_cat = pd.get_dummies(df[low_card_cols], dummy_na=True)\n",
    "\n",
    "    # Target 编码\n",
    "    if y is not None and high_card_cols:\n",
    "        encoder = ce.TargetEncoder()\n",
    "        X_target = encoder.fit_transform(df[high_card_cols], y)\n",
    "        X_target.columns = [f\"{col}_te\" for col in high_card_cols]\n",
    "    else:\n",
    "        X_target = pd.DataFrame(index=df.index)\n",
    "\n",
    "    # 数值标准化\n",
    "    num_cols = df.select_dtypes(include=[np.number]).columns\n",
    "    scaler = StandardScaler()\n",
    "    X_num_scaled = pd.DataFrame(scaler.fit_transform(df[num_cols]), columns=num_cols, index=df.index)\n",
    "\n",
    "    # 拼接所有\n",
    "    X_final_df = pd.concat([X_num_scaled, X_cat, X_target], axis=1)\n",
    "\n",
    "    return X_final_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "742452d8-9423-47c5-bdd7-f58acf4221f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_knn_price_features_radius(\n",
    "    df, base_df, lat_col='latitude', lon_col='longitude',\n",
    "    target_col='sale_price', radii=[0.005, 0.01, 0.02], max_neighbors=300\n",
    "):\n",
    "    df = df.copy()\n",
    "    coords_query = df[[lat_col, lon_col]].astype(np.float64).values\n",
    "    coords_base = base_df[[lat_col, lon_col]].astype(np.float64).values\n",
    "    targets_base = base_df[target_col].astype(np.float64).values\n",
    "\n",
    "    tree = KDTree(coords_base, leaf_size=40, metric='euclidean')\n",
    "\n",
    "    for r in radii:\n",
    "        neighbor_indices = tree.query_radius(coords_query, r=r)\n",
    "        \n",
    "        stats = {\n",
    "            'mean': [], 'std': [], 'min': [], 'max': [],\n",
    "            'range': [], 'iqr': [], 'median': [], 'count': []\n",
    "        }\n",
    "\n",
    "        for i, inds in enumerate(neighbor_indices):\n",
    "            if base_df is df:\n",
    "                inds = inds[inds != i]\n",
    "\n",
    "            if len(inds) == 0:\n",
    "                for key in stats:\n",
    "                    stats[key].append(np.nan if key != 'count' else 0)\n",
    "            else:\n",
    "                if len(inds) > max_neighbors:\n",
    "                    inds = inds[:max_neighbors]\n",
    "                vals = targets_base[inds]\n",
    "                stats['mean'].append(np.mean(vals))\n",
    "                stats['std'].append(np.std(vals))\n",
    "                stats['min'].append(np.min(vals))\n",
    "                stats['max'].append(np.max(vals))\n",
    "                stats['range'].append(np.max(vals) - np.min(vals))\n",
    "                stats['iqr'].append(np.percentile(vals, 75) - np.percentile(vals, 25))\n",
    "                stats['median'].append(np.median(vals))\n",
    "                stats['count'].append(len(inds))\n",
    "\n",
    "        for key in stats:\n",
    "            df[f'knn_radius_{key}_r{r}'] = stats[key]\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def add_knn_price_features(df, base_df=None, lat_col='latitude', lon_col='longitude',\n",
    "                           target_col='target', ks=[5, 10, 20]):\n",
    "    if base_df is None:\n",
    "        base_df = df  # 默认自己为近邻池\n",
    "\n",
    "    coords_query = df[[lat_col, lon_col]].values\n",
    "    coords_base = base_df[[lat_col, lon_col]].values\n",
    "    tree = KDTree(coords_base, metric='euclidean')\n",
    "\n",
    "    base_targets = base_df[target_col].values\n",
    "    knn_features = {}\n",
    "\n",
    "    for k in ks:\n",
    "        dists, indices = tree.query(coords_query, k=k)\n",
    "        neighbor_targets = base_targets[indices]\n",
    "\n",
    "        knn_features[f'knn_price_mean_{k}'] = neighbor_targets.mean(axis=1)\n",
    "        knn_features[f'knn_price_std_{k}'] = neighbor_targets.std(axis=1)\n",
    "        knn_features[f'knn_price_range_{k}'] = neighbor_targets.max(axis=1) - neighbor_targets.min(axis=1)\n",
    "\n",
    "    for col, val in knn_features.items():\n",
    "        df[col] = val\n",
    "\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3330bedc-0396-4f9b-88bc-cc93c09a9766",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(400000, 77)\n",
      "['knn_price_mean_5', 'knn_price_std_5', 'knn_price_range_5', 'knn_price_mean_10', 'knn_price_std_10', 'knn_price_range_10', 'knn_price_mean_20', 'knn_price_std_20', 'knn_price_range_20', 'knn_radius_mean_r0.005', 'knn_radius_std_r0.005', 'knn_radius_min_r0.005', 'knn_radius_max_r0.005', 'knn_radius_range_r0.005', 'knn_radius_iqr_r0.005', 'knn_radius_median_r0.005', 'knn_radius_count_r0.005', 'knn_radius_mean_r0.01', 'knn_radius_std_r0.01', 'knn_radius_min_r0.01', 'knn_radius_max_r0.01', 'knn_radius_range_r0.01', 'knn_radius_iqr_r0.01', 'knn_radius_median_r0.01', 'knn_radius_count_r0.01', 'knn_radius_mean_r0.02', 'knn_radius_std_r0.02', 'knn_radius_min_r0.02', 'knn_radius_max_r0.02', 'knn_radius_range_r0.02', 'knn_radius_iqr_r0.02', 'knn_radius_median_r0.02', 'knn_radius_count_r0.02']\n"
     ]
    }
   ],
   "source": [
    "# 应用预处理\n",
    "# 对训练集（自己做自己）\n",
    "train = add_knn_price_features(train, base_df=train, target_col='sale_price')\n",
    "# 对测试集（用训练集做 base）\n",
    "test = add_knn_price_features(test, base_df=train, target_col='sale_price')\n",
    "\n",
    "train = add_knn_price_features_radius(train, base_df=train, target_col='sale_price')\n",
    "# 对测试集（用训练集做 base）\n",
    "test = add_knn_price_features_radius(test, base_df=train, target_col='sale_price')\n",
    "\n",
    "train_features = train.drop(['sale_price'], axis=1)\n",
    "test_features = test\n",
    "\n",
    "features = pd.concat([train_features, test_features]).reset_index(drop=True)\n",
    "print(features.shape)\n",
    "print([col for col in features.columns if 'knn' in col])\n",
    "\n",
    "X_final = preprocess_and_encode(features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3a8585d4-ee9c-4577-aaf9-20952fe53012",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X (200000, 1156) y (200000,) X_sub (200000, 1156)\n",
      "['join_status_nan', 'submarket_nan']\n",
      "X (200000, 1154) y (200000,) X_sub (200000, 1154)\n"
     ]
    }
   ],
   "source": [
    "X = X_final.iloc[:len(train_features)].reset_index(drop=True)\n",
    "X_sub = X_final.iloc[len(train_features):].reset_index(drop=True)\n",
    "print('X', X.shape, 'y', y.shape, 'X_sub', X_sub.shape)\n",
    "\n",
    "protected_cols = []\n",
    "overfit = [i for i in X.columns if i not in protected_cols and X[i].value_counts().iloc[0] / len(X) * 100 > 99.94]\n",
    "print(overfit)\n",
    "\n",
    "X = X.drop(overfit, axis=1)\n",
    "X_sub = X_sub.drop(overfit, axis=1)\n",
    "print('X', X.shape, 'y', y.shape, 'X_sub', X_sub.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "87aa23db-668a-44f6-949d-c0edd5768e03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ========== 1. 定义网络结构 ==========\n",
    "class QuantileRegressor(nn.Module):\n",
    "    def __init__(self, input_dim):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(512),\n",
    "            nn.Dropout(0.3),\n",
    "\n",
    "            nn.Linear(512, 256),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(256),\n",
    "            nn.Dropout(0.2),\n",
    "\n",
    "            nn.Linear(256, 128),\n",
    "            nn.ReLU(),\n",
    "\n",
    "            nn.Linear(128, 2),  # 可以加入新层\n",
    "\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "\n",
    "\n",
    "# ========== 2. Quantile Loss (Pinball Loss) ==========\n",
    "def pinball_interval_loss(preds, target, alpha=0.8, coverage=1 ,penalty_weight=10.0):\n",
    "    y_low = preds[:, 0]\n",
    "    y_high = preds[:, 1]\n",
    "    y = target.squeeze()\n",
    "\n",
    "    # 覆盖惩罚\n",
    "    under = (y < y_low).float()\n",
    "    over = (y > y_high).float()\n",
    "    if coverage < 0.95:\n",
    "        #cover =((1-coverage)*250)\n",
    "        cover = 1.3\n",
    "\n",
    "    else: \n",
    "        cover = 1\n",
    "\n",
    "    #cover =((1-coverage)*20) + 1\n",
    "\n",
    "        \n",
    "        \n",
    "    miss_penalty = (2.0 / (1 - alpha)) * ((y_low - y) * under + (y - y_high) * over) * cover\n",
    "\n",
    "    # 区间宽度惩罚\n",
    "    width_penalty = (y_high - y_low)\n",
    "\n",
    "    # 结构惩罚项：low > high 是不合法的\n",
    "    structure_penalty = torch.mean(torch.relu(y_low - y_high)) * penalty_weight\n",
    "\n",
    "    return torch.mean(width_penalty + miss_penalty) + structure_penalty\n",
    "\n",
    "\n",
    "def interval_score_loss(preds, target, alpha=0.8):\n",
    "    y_low = preds[:, 0]\n",
    "    y_high = preds[:, 1]\n",
    "    y = target.squeeze()\n",
    "\n",
    "    width = y_high - y_low\n",
    "    below = (y < y_low).float()\n",
    "    above = (y > y_high).float()\n",
    "\n",
    "    under_penalty = (2 / alpha) * (y_low - y) * below\n",
    "    over_penalty = (2 / alpha) * (y - y_high) * above\n",
    "\n",
    "    score = width + under_penalty + over_penalty\n",
    "    return torch.mean(score)\n",
    "\n",
    "\n",
    "\n",
    "# ========== 3. 训练函数 ==========\n",
    "def train_model(X, y, alpha=0.8, epochs=200, batch_size=1024, lr=1e-3):\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    # 数据准备\n",
    "    X_train, X_val, y_train, y_val = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32)\n",
    "    X_val_tensor = torch.tensor(X_val, dtype=torch.float32)\n",
    "    y_val_tensor = torch.tensor(y_val, dtype=torch.float32)\n",
    "\n",
    "    train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=batch_size, num_workers=4, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(X_val_tensor, y_val_tensor), batch_size=batch_size)\n",
    "\n",
    "    # 模型定义\n",
    "    model = QuantileRegressor(input_dim=X.shape[1]).to(device)\n",
    "\n",
    "    #optimizer = torch.optim.AdamW(model.parameters(), lr=1e-2, weight_decay=1e-5)\n",
    "    #scheduler = ReduceLROnPlateau(optimizer, mode='min', factor=0.5, patience=3)\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=5e-4, weight_decay=1e-4)\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)\n",
    "\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "    best_model = None\n",
    "    patience, patience_counter = 10, 0\n",
    "\n",
    "    # ========== 4. 训练循环 ==========\n",
    "    coverage = 1\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for xb, yb in train_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            preds = model(xb)\n",
    "            loss = pinball_interval_loss(preds, yb, alpha, coverage)\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            total_loss += loss.item()\n",
    "            \n",
    "\n",
    "        # 验证\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            val_loss = 0\n",
    "            for xb, yb in val_loader:\n",
    "                xb, yb = xb.to(device), yb.to(device)\n",
    "                preds = model(xb)\n",
    "                loss = pinball_interval_loss(preds, yb, alpha, coverage)\n",
    "                val_loss += loss.item()\n",
    "\n",
    "\n",
    "        print(f\"Epoch {epoch+1}, Train Loss: {total_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "        with torch.no_grad():\n",
    "            preds = model(X_val_tensor.to(device)).cpu().numpy()\n",
    "            y_true = y_val_tensor.cpu().numpy()\n",
    "            coverage = np.mean((y_true >= preds[:, 0]) & (y_true <= preds[:, 1]))\n",
    "            avg_width = np.mean(preds[:, 1] - preds[:, 0])\n",
    "            print(f\"Val Coverage: {coverage:.4f}, Avg Width: {avg_width:.2f}\")\n",
    "\n",
    "        scheduler.step(val_loss)\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"Current LR: {current_lr:.6f}\")\n",
    "\n",
    "        \n",
    "        # Early Stopping\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            best_model = model.state_dict()\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "            if patience_counter >= patience:\n",
    "                print(\"Early stopping triggered!\")\n",
    "                break\n",
    "\n",
    "    model.load_state_dict(best_model)\n",
    "    torch.save(best_model, 'best_interval_model.pth')\n",
    "    return model\n",
    "\n",
    "\n",
    "# ========== 5. 用法示例 ==========\n",
    "# X_final: numpy 数组 (N, D)\n",
    "# y_train_lower, y_train_upper: numpy 数组 (N,)\n",
    "# 合并成 target 矩阵：\n",
    "# y_train = np.stack([y_train_lower, y_train_upper], axis=1)\n",
    "\n",
    "# model = train_model(X_final, y_train)\n",
    "\n",
    "# 输出预测\n",
    "# model(torch.tensor(X_test).to(device)).cpu().detach().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "1ce852da-6550-428e-91a1-d42b50c3be12",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = X.astype(np.float32)\n",
    "y = y.astype(np.float32)\n",
    "X_sub = X_sub.astype(np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "c7451698-cf85-488a-b3c5-33fed2ef4e6a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1, Train Loss: 4366.3718, Val Loss: 49.5555\n",
      "Val Coverage: 0.9823, Avg Width: 2.33\n",
      "Current LR: 0.000377\n",
      "Epoch 2, Train Loss: 538.3723, Val Loss: 38.7844\n",
      "Val Coverage: 0.9894, Avg Width: 1.89\n",
      "Current LR: 0.000422\n",
      "Epoch 3, Train Loss: 491.4649, Val Loss: 36.0798\n",
      "Val Coverage: 0.9827, Avg Width: 1.74\n",
      "Current LR: 0.000432\n",
      "Epoch 4, Train Loss: 505.0483, Val Loss: 40.9931\n",
      "Val Coverage: 0.9923, Avg Width: 2.02\n",
      "Current LR: 0.000413\n",
      "Epoch 5, Train Loss: 473.4961, Val Loss: 40.5140\n",
      "Val Coverage: 0.9950, Avg Width: 2.01\n",
      "Current LR: 0.000415\n",
      "Epoch 6, Train Loss: 477.9582, Val Loss: 38.6287\n",
      "Val Coverage: 0.9926, Avg Width: 1.91\n",
      "Current LR: 0.000423\n",
      "Epoch 7, Train Loss: 449.5636, Val Loss: 36.9536\n",
      "Val Coverage: 0.9919, Avg Width: 1.83\n",
      "Current LR: 0.000429\n",
      "Epoch 8, Train Loss: 453.4338, Val Loss: 43.7710\n",
      "Val Coverage: 0.9899, Avg Width: 2.17\n",
      "Current LR: 0.000402\n",
      "Epoch 9, Train Loss: 451.8171, Val Loss: 38.4587\n",
      "Val Coverage: 0.9953, Avg Width: 1.91\n",
      "Current LR: 0.000423\n",
      "Epoch 10, Train Loss: 451.8760, Val Loss: 35.1238\n",
      "Val Coverage: 0.9906, Avg Width: 1.74\n",
      "Current LR: 0.000435\n",
      "Epoch 11, Train Loss: 449.2773, Val Loss: 33.7121\n",
      "Val Coverage: 0.9869, Avg Width: 1.66\n",
      "Current LR: 0.000440\n",
      "Epoch 12, Train Loss: 435.2450, Val Loss: 34.2108\n",
      "Val Coverage: 0.9917, Avg Width: 1.69\n",
      "Current LR: 0.000439\n",
      "Epoch 13, Train Loss: 433.3563, Val Loss: 33.7482\n",
      "Val Coverage: 0.9881, Avg Width: 1.66\n",
      "Current LR: 0.000440\n",
      "Epoch 14, Train Loss: 426.9515, Val Loss: 33.9803\n",
      "Val Coverage: 0.9915, Avg Width: 1.68\n",
      "Current LR: 0.000439\n",
      "Epoch 15, Train Loss: 423.4273, Val Loss: 34.4156\n",
      "Val Coverage: 0.9916, Avg Width: 1.70\n",
      "Current LR: 0.000438\n",
      "Epoch 16, Train Loss: 416.6479, Val Loss: 34.1715\n",
      "Val Coverage: 0.9898, Avg Width: 1.69\n",
      "Current LR: 0.000439\n",
      "Epoch 17, Train Loss: 404.9742, Val Loss: 34.0425\n",
      "Val Coverage: 0.9905, Avg Width: 1.68\n",
      "Current LR: 0.000439\n",
      "Epoch 18, Train Loss: 409.0595, Val Loss: 35.5625\n",
      "Val Coverage: 0.9942, Avg Width: 1.77\n",
      "Current LR: 0.000434\n",
      "Epoch 19, Train Loss: 407.6582, Val Loss: 34.4584\n",
      "Val Coverage: 0.9887, Avg Width: 1.70\n",
      "Current LR: 0.000438\n",
      "Epoch 20, Train Loss: 408.4639, Val Loss: 34.3631\n",
      "Val Coverage: 0.9941, Avg Width: 1.71\n",
      "Current LR: 0.000438\n",
      "Epoch 21, Train Loss: 405.6034, Val Loss: 33.4562\n",
      "Val Coverage: 0.9885, Avg Width: 1.65\n",
      "Current LR: 0.000441\n",
      "Epoch 22, Train Loss: 399.2156, Val Loss: 34.2487\n",
      "Val Coverage: 0.9873, Avg Width: 1.69\n",
      "Current LR: 0.000438\n",
      "Epoch 23, Train Loss: 390.4568, Val Loss: 31.9936\n",
      "Val Coverage: 0.9879, Avg Width: 1.58\n",
      "Current LR: 0.000446\n",
      "Epoch 24, Train Loss: 383.8989, Val Loss: 32.9915\n",
      "Val Coverage: 0.9899, Avg Width: 1.63\n",
      "Current LR: 0.000443\n",
      "Epoch 25, Train Loss: 379.6136, Val Loss: 30.4244\n",
      "Val Coverage: 0.9873, Avg Width: 1.50\n",
      "Current LR: 0.000451\n",
      "Epoch 26, Train Loss: 377.2854, Val Loss: 31.0775\n",
      "Val Coverage: 0.9920, Avg Width: 1.54\n",
      "Current LR: 0.000449\n",
      "Epoch 27, Train Loss: 379.8956, Val Loss: 31.0082\n",
      "Val Coverage: 0.9917, Avg Width: 1.53\n",
      "Current LR: 0.000449\n",
      "Epoch 28, Train Loss: 371.1980, Val Loss: 29.3864\n",
      "Val Coverage: 0.9879, Avg Width: 1.45\n",
      "Current LR: 0.000454\n",
      "Epoch 29, Train Loss: 367.8937, Val Loss: 28.3597\n",
      "Val Coverage: 0.9874, Avg Width: 1.39\n",
      "Current LR: 0.000457\n",
      "Epoch 30, Train Loss: 356.0317, Val Loss: 27.6996\n",
      "Val Coverage: 0.9854, Avg Width: 1.36\n",
      "Current LR: 0.000459\n",
      "Epoch 31, Train Loss: 359.5571, Val Loss: 28.7282\n",
      "Val Coverage: 0.9894, Avg Width: 1.41\n",
      "Current LR: 0.000456\n",
      "Epoch 32, Train Loss: 350.0876, Val Loss: 27.7938\n",
      "Val Coverage: 0.9681, Avg Width: 1.34\n",
      "Current LR: 0.000459\n",
      "Epoch 33, Train Loss: 347.1879, Val Loss: 28.7263\n",
      "Val Coverage: 0.9870, Avg Width: 1.41\n",
      "Current LR: 0.000456\n",
      "Epoch 34, Train Loss: 346.9883, Val Loss: 29.5104\n",
      "Val Coverage: 0.9886, Avg Width: 1.45\n",
      "Current LR: 0.000454\n",
      "Epoch 35, Train Loss: 336.8540, Val Loss: 27.7518\n",
      "Val Coverage: 0.9744, Avg Width: 1.34\n",
      "Current LR: 0.000459\n",
      "Epoch 36, Train Loss: 334.9305, Val Loss: 27.0465\n",
      "Val Coverage: 0.9864, Avg Width: 1.32\n",
      "Current LR: 0.000461\n",
      "Epoch 37, Train Loss: 333.7166, Val Loss: 25.7084\n",
      "Val Coverage: 0.9807, Avg Width: 1.25\n",
      "Current LR: 0.000465\n",
      "Epoch 38, Train Loss: 331.8901, Val Loss: 26.7806\n",
      "Val Coverage: 0.9871, Avg Width: 1.31\n",
      "Current LR: 0.000462\n",
      "Epoch 39, Train Loss: 326.2603, Val Loss: 28.0795\n",
      "Val Coverage: 0.9861, Avg Width: 1.37\n",
      "Current LR: 0.000458\n",
      "Epoch 40, Train Loss: 317.6568, Val Loss: 27.0063\n",
      "Val Coverage: 0.9883, Avg Width: 1.32\n",
      "Current LR: 0.000461\n",
      "Epoch 41, Train Loss: 319.1403, Val Loss: 25.5560\n",
      "Val Coverage: 0.9857, Avg Width: 1.25\n",
      "Current LR: 0.000465\n",
      "Epoch 42, Train Loss: 308.0827, Val Loss: 25.9928\n",
      "Val Coverage: 0.9808, Avg Width: 1.26\n",
      "Current LR: 0.000464\n",
      "Epoch 43, Train Loss: 309.2631, Val Loss: 25.1991\n",
      "Val Coverage: 0.9800, Avg Width: 1.22\n",
      "Current LR: 0.000466\n",
      "Epoch 44, Train Loss: 300.6069, Val Loss: 24.9146\n",
      "Val Coverage: 0.9856, Avg Width: 1.22\n",
      "Current LR: 0.000467\n",
      "Epoch 45, Train Loss: 295.1770, Val Loss: 23.8395\n",
      "Val Coverage: 0.9840, Avg Width: 1.16\n",
      "Current LR: 0.000469\n",
      "Epoch 46, Train Loss: 286.1876, Val Loss: 23.5447\n",
      "Val Coverage: 0.9831, Avg Width: 1.14\n",
      "Current LR: 0.000470\n",
      "Epoch 47, Train Loss: 279.8479, Val Loss: 25.7161\n",
      "Val Coverage: 0.9861, Avg Width: 1.26\n",
      "Current LR: 0.000465\n",
      "Epoch 48, Train Loss: 277.9970, Val Loss: 22.2497\n",
      "Val Coverage: 0.9781, Avg Width: 1.08\n",
      "Current LR: 0.000473\n",
      "Epoch 49, Train Loss: 278.5619, Val Loss: 22.0917\n",
      "Val Coverage: 0.9799, Avg Width: 1.07\n",
      "Current LR: 0.000474\n",
      "Epoch 50, Train Loss: 270.1981, Val Loss: 22.7293\n",
      "Val Coverage: 0.9738, Avg Width: 1.10\n",
      "Current LR: 0.000472\n",
      "Epoch 51, Train Loss: 260.6903, Val Loss: 21.1275\n",
      "Val Coverage: 0.9637, Avg Width: 1.01\n",
      "Current LR: 0.000476\n",
      "Epoch 52, Train Loss: 252.5873, Val Loss: 19.7075\n",
      "Val Coverage: 0.9526, Avg Width: 0.92\n",
      "Current LR: 0.000479\n",
      "Epoch 53, Train Loss: 247.4694, Val Loss: 21.1009\n",
      "Val Coverage: 0.9469, Avg Width: 0.97\n",
      "Current LR: 0.000476\n",
      "Epoch 54, Train Loss: 261.0347, Val Loss: 23.4391\n",
      "Val Coverage: 0.9717, Avg Width: 1.11\n",
      "Current LR: 0.000470\n",
      "Epoch 55, Train Loss: 233.7904, Val Loss: 19.5606\n",
      "Val Coverage: 0.9676, Avg Width: 0.93\n",
      "Current LR: 0.000479\n",
      "Epoch 56, Train Loss: 229.8165, Val Loss: 18.9515\n",
      "Val Coverage: 0.9618, Avg Width: 0.89\n",
      "Current LR: 0.000481\n",
      "Epoch 57, Train Loss: 224.2110, Val Loss: 19.2180\n",
      "Val Coverage: 0.9588, Avg Width: 0.90\n",
      "Current LR: 0.000480\n",
      "Epoch 58, Train Loss: 216.1895, Val Loss: 17.8233\n",
      "Val Coverage: 0.9487, Avg Width: 0.82\n",
      "Current LR: 0.000483\n",
      "Epoch 59, Train Loss: 225.1816, Val Loss: 21.2931\n",
      "Val Coverage: 0.9594, Avg Width: 0.99\n",
      "Current LR: 0.000476\n",
      "Epoch 60, Train Loss: 203.7622, Val Loss: 16.5908\n",
      "Val Coverage: 0.9313, Avg Width: 0.74\n",
      "Current LR: 0.000485\n",
      "Epoch 61, Train Loss: 210.5696, Val Loss: 19.9598\n",
      "Val Coverage: 0.9609, Avg Width: 0.92\n",
      "Current LR: 0.000478\n",
      "Epoch 62, Train Loss: 186.6032, Val Loss: 19.1265\n",
      "Val Coverage: 0.9368, Avg Width: 0.86\n",
      "Current LR: 0.000480\n",
      "Epoch 63, Train Loss: 197.4478, Val Loss: 17.3843\n",
      "Val Coverage: 0.9326, Avg Width: 0.76\n",
      "Current LR: 0.000484\n",
      "Epoch 64, Train Loss: 189.8922, Val Loss: 17.4725\n",
      "Val Coverage: 0.9061, Avg Width: 0.73\n",
      "Current LR: 0.000483\n",
      "Epoch 65, Train Loss: 185.8184, Val Loss: 17.6775\n",
      "Val Coverage: 0.9393, Avg Width: 0.78\n",
      "Current LR: 0.000483\n",
      "Epoch 66, Train Loss: 177.8278, Val Loss: 16.7057\n",
      "Val Coverage: 0.9063, Avg Width: 0.69\n",
      "Current LR: 0.000485\n",
      "Epoch 67, Train Loss: 170.1035, Val Loss: 15.5601\n",
      "Val Coverage: 0.9300, Avg Width: 0.67\n",
      "Current LR: 0.000487\n",
      "Epoch 68, Train Loss: 161.9939, Val Loss: 16.6628\n",
      "Val Coverage: 0.9214, Avg Width: 0.71\n",
      "Current LR: 0.000485\n",
      "Epoch 69, Train Loss: 159.1174, Val Loss: 15.9183\n",
      "Val Coverage: 0.9328, Avg Width: 0.69\n",
      "Current LR: 0.000486\n",
      "Epoch 70, Train Loss: 156.2794, Val Loss: 14.7714\n",
      "Val Coverage: 0.9043, Avg Width: 0.60\n",
      "Current LR: 0.000488\n",
      "Epoch 71, Train Loss: 152.9041, Val Loss: 15.1115\n",
      "Val Coverage: 0.9034, Avg Width: 0.62\n",
      "Current LR: 0.000488\n",
      "Epoch 72, Train Loss: 147.1560, Val Loss: 14.7918\n",
      "Val Coverage: 0.8876, Avg Width: 0.58\n",
      "Current LR: 0.000488\n",
      "Epoch 73, Train Loss: 143.9619, Val Loss: 14.9831\n",
      "Val Coverage: 0.8874, Avg Width: 0.59\n",
      "Current LR: 0.000488\n",
      "Epoch 74, Train Loss: 142.0874, Val Loss: 14.5531\n",
      "Val Coverage: 0.9020, Avg Width: 0.59\n",
      "Current LR: 0.000488\n",
      "Epoch 75, Train Loss: 141.7855, Val Loss: 13.8595\n",
      "Val Coverage: 0.8802, Avg Width: 0.54\n",
      "Current LR: 0.000490\n",
      "Epoch 76, Train Loss: 136.5450, Val Loss: 13.8442\n",
      "Val Coverage: 0.8960, Avg Width: 0.55\n",
      "Current LR: 0.000490\n",
      "Epoch 77, Train Loss: 132.7645, Val Loss: 14.6671\n",
      "Val Coverage: 0.8599, Avg Width: 0.55\n",
      "Current LR: 0.000488\n",
      "Epoch 78, Train Loss: 132.3920, Val Loss: 13.8957\n",
      "Val Coverage: 0.8915, Avg Width: 0.55\n",
      "Current LR: 0.000489\n",
      "Epoch 79, Train Loss: 128.8957, Val Loss: 13.9442\n",
      "Val Coverage: 0.8561, Avg Width: 0.52\n",
      "Current LR: 0.000489\n",
      "Epoch 80, Train Loss: 127.2301, Val Loss: 13.0103\n",
      "Val Coverage: 0.8867, Avg Width: 0.50\n",
      "Current LR: 0.000491\n",
      "Epoch 81, Train Loss: 125.4423, Val Loss: 13.1048\n",
      "Val Coverage: 0.8853, Avg Width: 0.51\n",
      "Current LR: 0.000491\n",
      "Epoch 82, Train Loss: 124.7178, Val Loss: 13.1906\n",
      "Val Coverage: 0.9052, Avg Width: 0.53\n",
      "Current LR: 0.000491\n",
      "Epoch 83, Train Loss: 125.1824, Val Loss: 13.8916\n",
      "Val Coverage: 0.8749, Avg Width: 0.53\n",
      "Current LR: 0.000489\n",
      "Epoch 84, Train Loss: 123.9114, Val Loss: 13.4736\n",
      "Val Coverage: 0.8636, Avg Width: 0.51\n",
      "Current LR: 0.000490\n",
      "Epoch 85, Train Loss: 121.3598, Val Loss: 12.5865\n",
      "Val Coverage: 0.9002, Avg Width: 0.50\n",
      "Current LR: 0.000491\n",
      "Epoch 86, Train Loss: 118.2225, Val Loss: 12.4843\n",
      "Val Coverage: 0.8736, Avg Width: 0.47\n",
      "Current LR: 0.000492\n",
      "Epoch 87, Train Loss: 120.0742, Val Loss: 12.2923\n",
      "Val Coverage: 0.8959, Avg Width: 0.48\n",
      "Current LR: 0.000492\n",
      "Epoch 88, Train Loss: 117.0694, Val Loss: 12.5735\n",
      "Val Coverage: 0.8784, Avg Width: 0.47\n",
      "Current LR: 0.000491\n",
      "Epoch 89, Train Loss: 116.6412, Val Loss: 12.7393\n",
      "Val Coverage: 0.8516, Avg Width: 0.45\n",
      "Current LR: 0.000491\n",
      "Epoch 90, Train Loss: 117.9339, Val Loss: 12.2531\n",
      "Val Coverage: 0.8696, Avg Width: 0.45\n",
      "Current LR: 0.000492\n",
      "Epoch 91, Train Loss: 115.2179, Val Loss: 12.8007\n",
      "Val Coverage: 0.9046, Avg Width: 0.52\n",
      "Current LR: 0.000491\n",
      "Epoch 92, Train Loss: 114.9604, Val Loss: 12.4692\n",
      "Val Coverage: 0.9018, Avg Width: 0.50\n",
      "Current LR: 0.000492\n",
      "Epoch 93, Train Loss: 113.0371, Val Loss: 12.4328\n",
      "Val Coverage: 0.8684, Avg Width: 0.46\n",
      "Current LR: 0.000492\n",
      "Epoch 94, Train Loss: 114.3468, Val Loss: 12.1815\n",
      "Val Coverage: 0.8895, Avg Width: 0.47\n",
      "Current LR: 0.000492\n",
      "Epoch 95, Train Loss: 111.5873, Val Loss: 13.0731\n",
      "Val Coverage: 0.8666, Avg Width: 0.50\n",
      "Current LR: 0.000491\n",
      "Epoch 96, Train Loss: 112.8304, Val Loss: 12.2383\n",
      "Val Coverage: 0.8659, Avg Width: 0.45\n",
      "Current LR: 0.000492\n",
      "Epoch 97, Train Loss: 113.1332, Val Loss: 11.8676\n",
      "Val Coverage: 0.8724, Avg Width: 0.44\n",
      "Current LR: 0.000492\n",
      "Epoch 98, Train Loss: 110.5368, Val Loss: 12.6624\n",
      "Val Coverage: 0.8454, Avg Width: 0.43\n",
      "Current LR: 0.000491\n",
      "Epoch 99, Train Loss: 110.6520, Val Loss: 13.3038\n",
      "Val Coverage: 0.8831, Avg Width: 0.51\n",
      "Current LR: 0.000490\n",
      "Epoch 100, Train Loss: 110.7587, Val Loss: 12.3827\n",
      "Val Coverage: 0.8363, Avg Width: 0.43\n",
      "Current LR: 0.000492\n",
      "Epoch 101, Train Loss: 109.5415, Val Loss: 11.9104\n",
      "Val Coverage: 0.8732, Avg Width: 0.43\n",
      "Current LR: 0.000492\n",
      "Epoch 102, Train Loss: 108.2528, Val Loss: 11.8948\n",
      "Val Coverage: 0.8600, Avg Width: 0.43\n",
      "Current LR: 0.000492\n",
      "Epoch 103, Train Loss: 107.3668, Val Loss: 11.5564\n",
      "Val Coverage: 0.8662, Avg Width: 0.42\n",
      "Current LR: 0.000493\n",
      "Epoch 104, Train Loss: 106.5503, Val Loss: 11.8041\n",
      "Val Coverage: 0.8566, Avg Width: 0.42\n",
      "Current LR: 0.000492\n",
      "Epoch 105, Train Loss: 107.2411, Val Loss: 12.5812\n",
      "Val Coverage: 0.8692, Avg Width: 0.47\n",
      "Current LR: 0.000491\n",
      "Epoch 106, Train Loss: 105.2059, Val Loss: 11.9746\n",
      "Val Coverage: 0.8516, Avg Width: 0.42\n",
      "Current LR: 0.000492\n",
      "Epoch 107, Train Loss: 106.0467, Val Loss: 11.8503\n",
      "Val Coverage: 0.8969, Avg Width: 0.47\n",
      "Current LR: 0.000492\n",
      "Epoch 108, Train Loss: 105.1359, Val Loss: 11.5600\n",
      "Val Coverage: 0.8765, Avg Width: 0.43\n",
      "Current LR: 0.000493\n",
      "Epoch 109, Train Loss: 105.5293, Val Loss: 13.3554\n",
      "Val Coverage: 0.8906, Avg Width: 0.48\n",
      "Current LR: 0.000490\n",
      "Epoch 110, Train Loss: 104.2853, Val Loss: 11.5706\n",
      "Val Coverage: 0.8557, Avg Width: 0.41\n",
      "Current LR: 0.000493\n",
      "Epoch 111, Train Loss: 102.0784, Val Loss: 12.0927\n",
      "Val Coverage: 0.8475, Avg Width: 0.41\n",
      "Current LR: 0.000492\n",
      "Epoch 112, Train Loss: 102.7037, Val Loss: 11.6152\n",
      "Val Coverage: 0.8797, Avg Width: 0.43\n",
      "Current LR: 0.000493\n",
      "Epoch 113, Train Loss: 103.8127, Val Loss: 12.1699\n",
      "Val Coverage: 0.8317, Avg Width: 0.40\n",
      "Current LR: 0.000492\n",
      "Early stopping triggered!\n",
      "Finished training\n"
     ]
    }
   ],
   "source": [
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import math\n",
    "\n",
    "model = train_model(X.values, y.values, epochs=150)\n",
    "print(\"Finished training\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "28da964d-0908-4019-bbcb-833ff2945291",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finished prediction\n"
     ]
    }
   ],
   "source": [
    "X_sub = X_sub.astype(np.float32)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "pred = model(torch.tensor(X_sub.values).to(device)).cpu().detach().numpy()\n",
    "print(\"Finished prediction\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "9d85b775-33ec-424c-b65a-1bdc48f836bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_pred = pd.DataFrame(np.expm1(pred), columns=[\"pi_lower\", \"pi_upper\"])\n",
    "df_pred.insert(0, \"id\", test_ID)\n",
    "df_pred.to_csv(\"predictions4.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b9bcb8a8-e15a-42e5-a3a7-fe9955b259a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8192\n"
     ]
    }
   ],
   "source": [
    "print(4096*2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e0c9d89a-83af-4f06-b833-76a1945df811",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_dataset' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mcudnn\u001b[38;5;241m.\u001b[39mbenchmark \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m DataLoader(\u001b[43mtrain_dataset\u001b[49m, batch_size\u001b[38;5;241m=\u001b[39mbatch_size, shuffle\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, num_workers\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m4\u001b[39m, pin_memory\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'train_dataset' is not defined"
     ]
    }
   ],
   "source": [
    "torch.backends.cudnn.benchmark = True\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4, pin_memory=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f19d220-02cc-41a2-8fa2-b7457c0e75f2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
